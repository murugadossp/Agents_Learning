{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/murugadossp/Agents_Learning/blob/main/Quasi_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install litellm\n"
      ],
      "metadata": {
        "id": "KEYrzG2vB8Ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6661abac-6b2d-4592-cc95-1a069d990506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Collecting litellm',\n",
              " '  Downloading litellm-1.70.2-py3-none-any.whl.metadata (38 kB)',\n",
              " 'Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm) (3.11.15)',\n",
              " 'Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.2.0)',\n",
              " 'Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.28.1)',\n",
              " 'Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.7.0)',\n",
              " 'Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.6)',\n",
              " 'Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.23.0)',\n",
              " 'Collecting openai<1.76.0,>=1.68.2 (from litellm)',\n",
              " '  Downloading openai-1.75.0-py3-none-any.whl.metadata (25 kB)',\n",
              " 'Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.11.4)',\n",
              " 'Collecting python-dotenv>=0.2.0 (from litellm)',\n",
              " '  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)',\n",
              " 'Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.9.0)',\n",
              " 'Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.1)',\n",
              " 'Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (4.9.0)',\n",
              " 'Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (2025.4.26)',\n",
              " 'Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)',\n",
              " 'Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.10)',\n",
              " 'Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)',\n",
              " 'Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.21.0)',\n",
              " 'Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)',\n",
              " 'Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.3.0)',\n",
              " 'Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)',\n",
              " 'Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)',\n",
              " 'Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.24.0)',\n",
              " 'Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (1.9.0)',\n",
              " 'Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (0.9.0)',\n",
              " 'Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (1.3.1)',\n",
              " 'Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (4.67.1)',\n",
              " 'Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai<1.76.0,>=1.68.2->litellm) (4.13.2)',\n",
              " 'Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.7.0)',\n",
              " 'Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (2.33.2)',\n",
              " 'Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.0->litellm) (0.4.0)',\n",
              " 'Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)',\n",
              " 'Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)',\n",
              " 'Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (2.6.1)',\n",
              " 'Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.3.2)',\n",
              " 'Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.6.0)',\n",
              " 'Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (6.4.3)',\n",
              " 'Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (0.3.1)',\n",
              " 'Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm) (1.20.0)',\n",
              " 'Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.31.2)',\n",
              " 'Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.18.0)',\n",
              " 'Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.3.2)',\n",
              " 'Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (24.2)',\n",
              " 'Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)',\n",
              " 'Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.2)',\n",
              " 'Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.4.0)',\n",
              " 'Downloading litellm-1.70.2-py3-none-any.whl (7.9 MB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/7.9 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m2.3/7.9 MB\\x1b[0m \\x1b[31m67.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━\\x1b[0m \\x1b[32m6.6/7.9 MB\\x1b[0m \\x1b[31m94.4 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m7.9/7.9 MB\\x1b[0m \\x1b[31m95.7 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m7.9/7.9 MB\\x1b[0m \\x1b[31m65.2 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading openai-1.75.0-py3-none-any.whl (646 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/647.0 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m647.0/647.0 kB\\x1b[0m \\x1b[31m33.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)',\n",
              " 'Installing collected packages: python-dotenv, openai, litellm',\n",
              " '  Attempting uninstall: openai',\n",
              " '    Found existing installation: openai 1.78.1',\n",
              " '    Uninstalling openai-1.78.1:',\n",
              " '      Successfully uninstalled openai-1.78.1',\n",
              " 'Successfully installed litellm-1.70.2 openai-1.75.0 python-dotenv-1.1.0']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Important!!!\n",
        "#\n",
        "# <---- Set your 'OPENAI_API_KEY' as a secret over there with the \"key\" icon\n",
        "#\n",
        "#\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "epNTm5b7Ydsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "SqTweglJYbPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict"
      ],
      "metadata": {
        "id": "s1S3FFyMYhxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwe2eeOQB0cC",
        "outputId": "e9bf8221-7462-48d3-f8c7-5af33ab6457b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?I need help in fixing my Word crash\n",
            "I'm sorry to hear that Word is crashing. One effective solution is to turn your computer off and then back on. This can often resolve issues by clearing temporary glitches and refreshing the system. Please give it a try and see if it helps with the Word crashes.\n"
          ]
        }
      ],
      "source": [
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"openai/gpt-4o\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETk96tScY4P8",
        "outputId": "e89ca5a4-356d-4c7a-858b-ead567450016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry you're experiencing issues with Word crashing. A simple solution that often helps is to turn your computer off and then back on. This can refresh your system and may resolve the issue. Let me know if you need further assistance!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f39coi5bZBlU",
        "outputId": "0e3348b3-08a7-4ba5-e340-2d3c4f52dcf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What do you need help with?powerpoint problem\n",
            "It sounds like you're encountering an issue with PowerPoint. A great first step in troubleshooting this is to turn your computer off and then back on. This can often resolve software-related issues. If you're still having problems after restarting, let me know, and we can go from there!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes:\n",
        "\n",
        "System messages are particularly important in the conversation and will be very important for AI agents. They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can “program” the AI agent through system messages.\n",
        "\n",
        "Let’s simulate a customer service interaction for a customer service agent that always tells the customer to turn off their computer or modem with system messages:\n",
        "\n",
        "System messages are particularly important in the conversation and will be very important for AI agents. They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can “program” the AI agent through system messages.\n",
        "\n",
        "Let’s simulate a customer service interaction for a customer service agent that always tells the customer to turn off their computer or modem with system messages:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I get my Internet working again.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "The system message is the most important part of this prompt. It tells the model how to behave. The user message is the question that we want the model to answer. The system instructions lay the ground rules for the interaction.\n",
        "\n",
        "The messages can incorporate arbitrary information as long as it is in text form. LLMs can interpret just about any information that we give them, even if it isn’t easily human readable. Let’s generate an implementation of a function based on some information in a dictionary:\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "code_spec = {\n",
        "    'name': 'swap_keys_values',\n",
        "    'description': 'Swaps the keys and values in a given dictionary.',\n",
        "    'params': {\n",
        "        'd': 'A dictionary with unique values.'\n",
        "    },\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"You are an expert software engineer that writes clean functional code. You always document your functions.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Please implement: {json.dumps(code_spec)}\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "We will rely heavily on the ability to send the LLM just about any type of information, particularly JSON, when we start building agents. This is a simple example of how we can use JSON to send information to the LLM, but you can see how we could provide it JSON with information about the result of an API call, for example."
      ],
      "metadata": {
        "id": "kHzIoqPuZxrp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cSn3EOWDZHhO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}